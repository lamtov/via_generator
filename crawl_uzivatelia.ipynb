{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:979: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.mojevideo.sk'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "req = requests.get('https://www.mojevideo.sk/uzivatelia/26/', verify=False)\n",
    "req.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(req.text, 'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hazard',\n",
       " 'posledneslovo',\n",
       " 'jasch381',\n",
       " 'jpegasussp',\n",
       " 'patki',\n",
       " 'matho11',\n",
       " 'darkmoon',\n",
       " 'duen969',\n",
       " 'superuser',\n",
       " 'martin55',\n",
       " 'trojanhorsee',\n",
       " 'svenik',\n",
       " 'matt251',\n",
       " 'jurajo',\n",
       " 'dejvoo',\n",
       " 'brutal',\n",
       " 'gacoo',\n",
       " 'styrko',\n",
       " 'urguhart',\n",
       " 'crocodiljoe',\n",
       " 'hladimir',\n",
       " 'krtko1985',\n",
       " 'rajen17',\n",
       " 'darcaorganov',\n",
       " 'turkmenpasa',\n",
       " 'pistacius',\n",
       " 'jochino',\n",
       " 'kasai',\n",
       " 'definer',\n",
       " 'aladarrr']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_eps_td(tag):\n",
    "    return tag.name=='td' and (len(tag.contents)>0 and tag.contents[0].name=='b')\n",
    "td_has_string=soup.find_all(find_eps_td)\n",
    "td_has_string\n",
    "list_name=[td.contents[0].string for td in td_has_string]\n",
    "list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "from threading import Lock\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "def find_eps_td(tag):\n",
    "    return tag.name=='td' and (len(tag.contents)>0 and tag.contents[0].name=='b')\n",
    "def get_next_page():\n",
    "    global count_page\n",
    "    global lock_1\n",
    "    try:\n",
    "        lock_1.acquire()\n",
    "        count_page=count_page+1\n",
    "        return count_page\n",
    "    finally:\n",
    "        lock_1.release()\n",
    "def find_lis_name_mojevideo(page):\n",
    "    global tag\n",
    "    global output\n",
    "    try:\n",
    "        req = requests.get('https://www.mojevideo.sk/'+ str(tag)+'/'+str(page)+'/', verify=False)\n",
    "        req.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(req.text, 'lxml')\n",
    "        td_has_string=soup.find_all(find_eps_td)\n",
    "        \n",
    "        if len(td_has_string) == 0:\n",
    "            stop = True\n",
    "        else:\n",
    "            list_name=[td.contents[0].string for td in td_has_string]\n",
    "            for line in list_name:\n",
    "                output.writelines(line+'\\n')\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "def thread_find_lis_name_mojevideo():\n",
    "    global count_page\n",
    "    try:\n",
    "        while stop is False:\n",
    "            page=get_next_page()\n",
    "            print(page)\n",
    "            find_lis_name_mojevideo(page)\n",
    "    finally:\n",
    "        output.close()\n",
    "def write_list_name_mojevideo( number_thread):\n",
    "    for i in range(0, number_thread):\n",
    "        try:\n",
    "            executor = ThreadPoolExecutor(1)\n",
    "            executor.submit(thread_find_lis_name_mojevideo)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    stop = False\n",
    "    tag='uzivatelia'\n",
    "    if not os.path.exists(cur_dir+'/write_list_name_mojevideo'):\n",
    "        os.makedirs(cur_dir+'/write_list_name_mojevideo')\n",
    "    output = open(cur_dir+'/write_list_name_mojevideo/' +'output_'+str(tag)+'.txt', 'w', encoding='utf-8' )\n",
    "    lock_1 = Lock()\n",
    "    count_page=0\n",
    "    number_thread=8\n",
    "    write_list_name_mojevideo( number_thread)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[889844, 800874], [800874, 711904], [711904, 622934], [622934, 533964], [533964, 444994], [444994, 356024], [356024, 267054], [267054, 178084], [178084, 89114], [89114, 0]]\n"
     ]
    }
   ],
   "source": [
    "def split(_len,_distance, _number_thread):\n",
    "    total_step=int(_len/_distance)\n",
    "    step_per_thread=int(total_step/_number_thread)\n",
    "    output=[]\n",
    "    for i in range(0,_number_thread):\n",
    "        if i<_number_thread-1:\n",
    "            output.append([i*step_per_thread*_distance,(i+1)*step_per_thread*_distance])\n",
    "        else:\n",
    "            output.append([i*step_per_thread*_distance,_len])\n",
    "    print(output)\n",
    "    \n",
    "def split_inverse(_len,_distance, _number_thread):\n",
    "    total_step=int(_len/_distance)\n",
    "    step_per_thread=int(total_step/_number_thread)\n",
    "    output=[]\n",
    "    for i in range(0,_number_thread):\n",
    "        if i<_number_thread-1:\n",
    "            output.append([_len-i*step_per_thread*_distance,_len-(i+1)*step_per_thread*_distance])\n",
    "        else:\n",
    "            output.append([_len-i*step_per_thread*_distance,_len-_len])\n",
    "    print(output)\n",
    "    \n",
    "split_inverse(889844,31,10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "from threading import Lock\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "cur_dir = os.getcwd()\n",
    "if not os.path.exists(cur_dir+'/write_list_name_modrykonik'):\n",
    "    os.makedirs(cur_dir+'/write_list_name_modrykonik')\n",
    "output = open(cur_dir+'/write_list_name_modrykonik/' +'output_modrykonik'+'.txt', 'w', encoding='utf-8' )\n",
    "def split_inverse(_len,_distance, _number_thread):\n",
    "    total_step=int(_len/_distance)\n",
    "    step_per_thread=int(total_step/_number_thread)\n",
    "    output=[]\n",
    "    for i in range(0,_number_thread):\n",
    "        if i<_number_thread-1:\n",
    "            output.append([_len-i*step_per_thread*_distance,_len-(i+1)*step_per_thread*_distance])\n",
    "        else:\n",
    "            output.append([_len-i*step_per_thread*_distance,_len-_len])\n",
    "    return output\n",
    "def get_list_user(start_end):\n",
    "    \n",
    "    start_num=start_end[0]\n",
    "    end_num=start_end[1]\n",
    "    output = open(cur_dir+'/write_list_name_modrykonik/' +'output_modrykonik'+str(start_num)+'_'+str(end_num)+'.txt', 'w', encoding='utf-8' )\n",
    "    while start_num > end_num:\n",
    "        try:\n",
    "            url = 'https://www.modrykonik.sk/api/forum/feed/new-feed/?after='+str(start_num)+'&orderby=newest'\n",
    "            headers = {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:86.0) Gecko/20100101 Firefox/86.0\",\n",
    "                    \"Accept\": \"*/*\",\n",
    "                    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "                    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "                    \"X-CSRFToken\": \"i0HZmfXT6kcTfCb3yQ4nd6KawsBiaHo5nNPVCZQaQmNy3sxv4zWU8Xe5s7KZHseD\",\n",
    "                    \"X-SSR-Request-Id\": \"89e21c866e65e183e9dd5e7daf250be9\"\n",
    "                }\n",
    "            r = requests.get(url, headers=headers)\n",
    "            start_num=int(r.json()['hasNext'])\n",
    "            print(start_num)\n",
    "            list_post=r.json()['posts']\n",
    "            list_user=[post['author']['username'] for post in list_post]\n",
    "            for user in list_user:\n",
    "#                 print(user)\n",
    "                output.writelines(str(user)+'\\n')    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            start_num=start_num-31\n",
    "    if end_num==889720:\n",
    "#         time. sleep(10) \n",
    "        output.close()\n",
    "number_thread=1\n",
    "distance=31\n",
    "start_index=889844\n",
    "per_thread=split_inverse(start_index,distance,number_thread)\n",
    "# print(per_thread)\n",
    "per_thread=[[889844, 2968]]\n",
    "for i in range(0, number_thread):\n",
    "    try:\n",
    "        print(per_thread[i])\n",
    "        executor = ThreadPoolExecutor(1)\n",
    "        executor.submit(get_list_user,per_thread[i])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# get_list_user([889844, 889782])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "from threading import Lock\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def remove_non_ascii(text):\n",
    "    return unidecode(text)\n",
    "cur_dir = os.getcwd()\n",
    "file1 = open(cur_dir+'/write_list_name_modrykonik/' +'output_modrykonik'+str(889844)+'_'+str(2968)+'.txt',  encoding='utf-8' )\n",
    "Lines = file1.readlines()\n",
    "list_origin=[]\n",
    "for line in Lines:\n",
    "    list_origin.append(line)\n",
    "\n",
    "list_origin = list(set(list_origin)) \n",
    "output = open(cur_dir+'/write_list_name_modrykonik/'+'file_user_data_modrykonik.txt', 'w', encoding='utf-8')\n",
    "for line in list_origin:\n",
    "    line=line\n",
    "    output.writelines(line)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết Quả Kinh Doanh ['Q2 2020', 'Q1 2020', 'Q4 2019', 'Q3 2019', 'Q2 2019', 'Q1 2019', 'Q4 2018', 'Q3 2018', 'Q2 2018', 'Q1 2018', 'Q4 2017', 'Q3 2017', 'Q2 2017', 'Q1 2017', 'Q4 2016', 'Q3 2016', 'Q2 2016', 'Q1 2016', 'Q4 2015', 'Q3 2015', 'Q2 2015', 'Q1 2015', 'Q4 2014', 'Q3 2014', 'Q2 2014', 'Q1 2014', 'Q4 2013', 'Q3 2013', 'Q2 2013', 'Q1 2013', 'Q4 2012', 'Q3 2012', 'Q2 2012', 'Q1 2012', 'Q4 2011', 'Q3 2011', 'Q2 2011', 'Q1 2011', 'Q4 2010', 'Q3 2010', 'Q2 2010', 'Q1 2010']\n"
     ]
    }
   ],
   "source": [
    "def get_list_ket_qua_kinh_doanh(soup ):\n",
    "    td_has_string=soup.find_all('td', string='Kết Quả Kinh Doanh')\n",
    "    list_td_quy= td_has_string[0].parent.contents\n",
    "    list_td=[td for td in list_td_quy if td.name=='td' and len(td.contents)>0]\n",
    "    name = list_td[0].string\n",
    "    values=[]\n",
    "    for i in range(1, len(list_td)):\n",
    "        values.append(list_td[i].string)\n",
    "    return name, values\n",
    "\n",
    "name,values=get_list_ket_qua_kinh_doanh(soup)\n",
    "print(name, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lợi nhuận sau thuế thu nhập doanh nghiệp ['80,721', '66,832', '68,870', '83,798', '149,221', '233,213', '55,549', '52,914', '50,703', '70,782', '85,363', '82,895', '58,341', '76,787', '41,449', '39,130', '33,888', '28,459', '21,303', '19,891', '5,791', '-6,436', '825', '16,581', '15,530', '14,844', '11,412', '13,360', '13,281', '17,418', '18,573', '11,348', '10,728', '15,529', '20,814', '18,913', '9,174', '16,770', '19,924', '41,887', '17,168', '11,886']\n"
     ]
    }
   ],
   "source": [
    "def get_list_loi_nhuan_sau_thue(soup ):\n",
    "    td_has_string=soup.find_all('td', string='Lợi nhuận sau thuế thu nhập doanh nghiệp')\n",
    "    list_td= td_has_string[0].parent.contents\n",
    "    list_td=[td for td in list_td if td.name=='td' and len(td.contents)>0]\n",
    "    name = list_td[0].string\n",
    "    values=[]\n",
    "# list_td\n",
    "    for i in range(1, len(list_td)):\n",
    "        values.append(list_td[i].contents[0].strip())\n",
    "    return name, values\n",
    "\n",
    "name,values=get_list_loi_nhuan_sau_thue(soup)\n",
    "print(name, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPS ['1,603', '1,990', '2,931', '2,870', '2,719', '2,113', '1,157', '1,284.3', '1,398.5', '1,469.4', '1,538.4', '1,339.7', '1,171.2', '1,083.4', '829.4', '711.8', '602.1', '438.3', '233.6', '113.8', '93.5', '149.9', '274.3', '337.1', '318.6', '306.0', '320.8', '359.8', '347.9', '333.2', '322.1', '337.9', '382.7', '374.0', '382.5', '377.6', '512.3', '559.3', '530.7', '414.4', '169.7', '69.5']\n"
     ]
    }
   ],
   "source": [
    "def find_eps_td(tag):\n",
    "    return tag.name=='td' and (len(tag.contents)>0 and tag.contents[0].string=='EPS')\n",
    "def get_list_eps(soup):\n",
    "    td_has_string=soup.find_all(find_eps_td)\n",
    "    list_td= td_has_string[0].parent.contents\n",
    "    list_td=[td for td in list_td if td.name=='td' and len(td.contents)>0]\n",
    "    name = 'EPS'\n",
    "    values=[]\n",
    "    for i in range(1, len(list_td)):\n",
    "        values.append(list_td[i].contents[0].strip())\n",
    "    return name, values\n",
    "\n",
    "\n",
    "name,values=get_list_eps(soup)\n",
    "print(name, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE ['6.9', '4.3', '4.0', '4.9', '6.0', '7.1', '11.3', '11.8', '11.7', '5.6', '4.9', '5.8', '6.3', '4.9', '5.3', '7.8', '9.0', '6.4', '8.7', '12.0', '14.8', '8.9', '5.4', '2.5', '2.3', '3.0', '2.1', '1.5', '1.5', '1.5', '1.6', '0.7', '0.7', '0.8', '0.5', '0.7', '0.5', '0.6', '0.9', '1.9', 'N/A', 'N/A']\n"
     ]
    }
   ],
   "source": [
    "def find_pe_td(tag):\n",
    "    return tag.name=='td' and (len(tag.contents)>0 and tag.contents[0].string=='PE')\n",
    "def get_list_pe(soup):\n",
    "    td_has_string=soup.find_all(find_pe_td)\n",
    "    list_td= td_has_string[0].parent.contents\n",
    "    list_td=[td for td in list_td if td.name=='td' and len(td.contents)>0]\n",
    "    name = 'PE'\n",
    "    values=[]\n",
    "    for i in range(1, len(list_td)):\n",
    "        values.append(list_td[i].string.strip())\n",
    "    return name, values\n",
    "\n",
    "\n",
    "name,values=get_list_pe(soup)\n",
    "print(name, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data={}\n",
    "name,values=get_list_ket_qua_kinh_doanh(soup)\n",
    "json_data[name]=values\n",
    "name,values=get_list_loi_nhuan_sau_thue(soup)\n",
    "json_data[name]=values\n",
    "name,values=get_list_eps(soup)\n",
    "json_data[name]=values\n",
    "name,values=get_list_pe(soup)\n",
    "json_data[name]=values\n",
    "# json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame(json_data)\n",
    "# data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py:979: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.55.123.98'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_companys=[\"AAA\", \"ABI\", \"ABS\", \"ACB\", \"ACV\", \"ADG\", \"AGG\", \"AGR\", \"AMD\", \"ANV\", \"ASM\", \"AST\", \"BCC\", \"BCG\", \"BCM\", \"BFC\", \"BHN\", \"BIC\", \"BID\", \"BMI\", \"BMP\", \"BOT\", \"BSA\", \"BSI\", \"BSR\", \"BVH\", \"BVS\", \"BWE\", \"C4G\", \"CAV\", \"CC1\", \"CEE\", \"CEO\", \"CHP\", \"CII\", \"CLC\", \"CLL\", \"CMG\", \"CNG\", \"CRE\", \"CSC\", \"CSM\", \"CSV\", \"CTD\", \"CTF\", \"CTG\", \"CTI\", \"CTR\", \"CTS\", \"CVT\", \"D2D\", \"DBC\", \"DBD\", \"DCL\", \"DCM\", \"DDV\", \"DGC\", \"DGW\", \"DHA\", \"DHC\", \"DHG\", \"DHT\", \"DIG\", \"DMC\", \"DNA\", \"DNP\", \"DPG\", \"DPM\", \"DPR\", \"DQC\", \"DRC\", \"DRL\", \"DSN\", \"DSP\", \"DVN\", \"DVP\", \"DXG\", \"EIB\", \"EVF\", \"FCN\", \"FIR\", \"FIT\", \"FLC\", \"FMC\", \"FOX\", \"FPT\", \"FTS\", \"GAB\", \"GAS\", \"GEG\", \"GEX\", \"GMD\", \"GTN\", \"GVR\", \"HAG\", \"HAH\", \"HAI\", \"HBC\", \"HCM\", \"HDB\", \"HDC\", \"HDG\", \"HHC\", \"HHS\", \"HNG\", \"HPG\", \"HPI\", \"HPX\", \"HQC\", \"HRC\", \"HSG\", \"HT1\", \"HTM\", \"HTN\", \"HVG\", \"HVN\", \"IBC\", \"IDC\", \"IDI\", \"IDJ\", \"IDV\", \"IJC\", \"IMP\", \"ITA\", \"ITC\", \"KBC\", \"KDC\", \"KDF\", \"KDH\", \"KOS\", \"KSB\", \"L14\", \"LAS\", \"LCG\", \"LDG\", \"LGC\", \"LHG\", \"LIX\", \"LPB\", \"LTG\", \"MBB\", \"MBS\", \"MCH\", \"MEG\", \"MIG\", \"MML\", \"MPC\", \"MSH\", \"MSN\", \"MSR\", \"MWG\", \"NAF\", \"NBB\", \"NCT\", \"ND2\", \"NDN\", \"NET\", \"NKG\", \"NLG\", \"NNC\", \"NT2\", \"NTC\", \"NTL\", \"NTP\", \"NVB\", \"NVL\", \"NVT\", \"OGC\", \"OIL\", \"PAC\", \"PAN\", \"PC1\", \"PDN\", \"PDR\", \"PET\", \"PGC\", \"PGD\", \"PGI\", \"PGS\", \"PHP\", \"PHR\", \"PLC\", \"PLX\", \"PME\", \"PMG\", \"PNJ\", \"POM\", \"POW\", \"PPC\", \"PTB\", \"PTI\", \"PVD\", \"PVI\", \"PVP\", \"PVS\", \"PVT\", \"PXL\", \"QCG\", \"QNS\", \"RAL\", \"REE\", \"ROS\", \"SAB\", \"SAM\", \"SAS\", \"SBA\", \"SBM\", \"SBT\", \"SCR\", \"SCS\", \"SDI\", \"SEA\", \"SGN\", \"SGP\", \"SGR\", \"SHB\", \"SHI\", \"SHN\", \"SHS\", \"SIP\", \"SJD\", \"SJS\", \"SKG\", \"SKV\", \"SLS\", \"SMB\", \"SMC\", \"SNZ\", \"SRC\", \"SSI\", \"STB\", \"STG\", \"STK\", \"SVC\", \"SVG\", \"SVI\", \"SWC\", \"SZB\", \"SZC\", \"SZL\", \"TAC\", \"TAR\", \"TBD\", \"TCB\", \"TCH\", \"TCM\", \"TDC\", \"TDH\", \"TDM\", \"TDP\", \"THG\", \"THI\", \"TIG\", \"TIS\", \"TIX\", \"TLG\", \"TMG\", \"TMP\", \"TNA\", \"TNG\", \"TNI\", \"TPB\", \"TRA\", \"TRC\", \"TV2\", \"TVB\", \"TVC\", \"TVN\", \"TVS\", \"VBB\", \"VC3\", \"VCB\", \"VCF\", \"VCG\", \"VCI\", \"VCP\", \"VCS\", \"VCW\", \"VDS\", \"VEA\", \"VEF\", \"VGC\", \"VGG\", \"VGI\", \"VGR\", \"VGT\", \"VHC\", \"VHM\", \"VIB\", \"VIC\", \"VIF\", \"VIS\", \"VIX\", \"VJC\", \"VLB\", \"VLC\", \"VNB\", \"VND\", \"VNG\", \"VNM\", \"VNS\", \"VOC\", \"VPB\", \"VPD\", \"VPG\", \"VPI\", \"VRE\", \"VSC\", \"VSH\", \"VTO\", \"VTP\", \"VTR\", \"WSB\", \"XHC\", \"YEG\"]\n",
    "list_id = [item.lower() for item in list_companys]\n",
    "list_id_fail=[]\n",
    "for _id in list_id[1:2]:\n",
    "    req = requests.get('https://www.cophieu68.vn/incomestatement.php?id='+_id+'&view=ist&year=20', verify=False)\n",
    "    req.encoding = 'utf-8'\n",
    "    if req.status_code !=200:\n",
    "        list_id_fail.append({_id:req.status_code})\n",
    "list_id_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py:979: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.55.123.98'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Kết Quả Kinh Doanh Lợi nhuận sau thuế thu nhập doanh nghiệp    EPS    PE\n",
      "0             Q2 2020                                   64,602  6,708   4.1\n",
      "1             Q1 2020                                   81,821  6,879   3.3\n",
      "2             Q4 2019                                   34,651  6,384   4.6\n",
      "3             Q3 2019                                   73,813  6,887   4.1\n",
      "4             Q2 2019                                   71,129  5,669   4.5\n",
      "5             Q1 2019                                   63,004  5,212   4.3\n",
      "6             Q4 2018                                   53,760  4,669   4.8\n",
      "7             Q3 2018                                   27,515  2,825   8.6\n",
      "8             Q2 2018                                   53,760  3,471   6.4\n",
      "9             Q1 2018                                   42,372  3,281   7.6\n",
      "10            Q4 2017                                  -16,302  3,380   7.0\n",
      "11            Q3 2017                                   52,077  4,407   5.7\n",
      "12            Q2 2017                                   46,521  3,985   7.9\n",
      "13            Q1 2017                                   46,128  3,571   6.4\n",
      "14            Q4 2016                                   22,753  2,934   6.6\n",
      "15            Q3 2016                                   36,014  2,888   5.2\n",
      "16            Q2 2016                                   30,804  2,501   6.2\n",
      "17            Q1 2016                                   21,909  2,246   5.5\n",
      "18            Q4 2015                                   21,023  2,153   4.4\n",
      "19            Q3 2015                                   21,312  3,026   3.1\n",
      "20            Q2 2015                                   21,104  3,072   2.6\n",
      "21            Q1 2015                                   18,388  3,020   2.9\n",
      "22            Q4 2014                                   54,179  3,076   2.2\n",
      "23            Q3 2014                                   23,082  1,871   3.5\n",
      "24            Q2 2014                                   19,120  1,532   3.7\n",
      "25            Q1 2014                                   20,499  1,029   5.4\n",
      "26            Q4 2013                                    8,382    490   8.6\n",
      "27            Q3 2013                                   10,221    269  14.7\n"
     ]
    }
   ],
   "source": [
    "for _id in list_id[1:2]:\n",
    "    req = requests.get('https://www.cophieu68.vn/incomestatement.php?id='+_id+'&view=ist&year=20', verify=False)\n",
    "    req.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    json_data={}\n",
    "    name,values=get_list_ket_qua_kinh_doanh(soup)\n",
    "    json_data[name]=values\n",
    "    name,values=get_list_loi_nhuan_sau_thue(soup)\n",
    "    json_data[name]=values\n",
    "    name,values=get_list_eps(soup)\n",
    "    json_data[name]=values\n",
    "    name,values=get_list_pe(soup)\n",
    "    json_data[name]=values\n",
    "    data_frame = pd.DataFrame(json_data)\n",
    "#     data_frame.to_csv(\"loi_nhuan_eps_pe_id=\"+_id+'.csv', index=False)\n",
    "    data_frame.to_excel(\"loi_nhuan_eps_pe_id=\"+_id+'.xlsx', index=False)\n",
    "    print(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
